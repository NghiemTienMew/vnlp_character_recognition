"""
test_single_image.py - Test model VOCR vá»›i Ä‘áº§u vÃ o lÃ  1 áº£nh
"""
import os
import sys
import argparse
import torch
import cv2
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path

# ThÃªm Ä‘Æ°á»ng dáº«n root vÃ o sys.path
root_path = Path(__file__).parent.parent
sys.path.append(str(root_path))

from config.config import get_config, load_config
from models.vocr import VOCR
from data.preprocessor import Preprocessor
from utils.visualize import visualize_predictions


def load_trained_model(checkpoint_path: str, config):
    """
    Load model Ä‘Ã£ Ä‘Æ°á»£c train
    
    Args:
        checkpoint_path: ÄÆ°á»ng dáº«n checkpoint
        config: Configuration
        
    Returns:
        model: Model Ä‘Ã£ load weights
    """
    try:
        # Táº¡o model
        model = VOCR()
        
        # Load checkpoint
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        
        print(f"âœ… Model loaded from: {checkpoint_path}")
        print(f"ğŸ“Š Model info: {model.get_model_info()}")
        
        if 'epoch' in checkpoint:
            print(f"ğŸ”„ Epoch: {checkpoint['epoch'] + 1}")
        if 'best_val_loss' in checkpoint:
            print(f"ğŸ“‰ Best validation loss: {checkpoint['best_val_loss']:.4f}")
        
        return model
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None


def preprocess_single_image(image_path: str, target_size=(32, 140)):
    """
    Tiá»n xá»­ lÃ½ 1 áº£nh Ä‘áº§u vÃ o
    
    Args:
        image_path: ÄÆ°á»ng dáº«n áº£nh
        target_size: KÃ­ch thÆ°á»›c target (height, width)
        
    Returns:
        torch.Tensor: áº¢nh Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½
    """
    try:
        # Äá»c áº£nh
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        # Äá»c áº£nh báº±ng OpenCV
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        
        if image is None:
            raise ValueError(f"Cannot read image: {image_path}")
        
        print(f"ğŸ“· Original image shape: {image.shape}")
        
        # Resize áº£nh
        image_resized = cv2.resize(image, (target_size[1], target_size[0]))
        print(f"ğŸ”„ Resized to: {image_resized.shape}")
        
        # Sá»­ dá»¥ng preprocessor
        preprocessor = Preprocessor()
        image_tensor = preprocessor(image_resized)
        
        # ThÃªm batch dimension
        image_tensor = image_tensor.unsqueeze(0)  # [1, 1, height, width]
        
        print(f"âœ… Preprocessed tensor shape: {image_tensor.shape}")
        
        return image_tensor, image_resized
        
    except Exception as e:
        print(f"âŒ Error preprocessing image: {e}")
        return None, None


def format_license_plate(text: str) -> str:
    """
    Format license plate vá»›i dáº¥u gáº¡ch ngang
    
    Args:
        text: License plate text
        
    Returns:
        str: Formatted text vá»›i dáº¥u gáº¡ch ngang
    """
    if not isinstance(text, str):
        text = str(text)
    
    # LÃ m sáº¡ch text
    text = text.replace(' ', '').upper().strip()
    
    # Náº¿u Ä‘Ã£ cÃ³ dáº¥u gáº¡ch ngang, giá»¯ nguyÃªn
    if '-' in text:
        return text
    
    # ThÃªm dáº¥u gáº¡ch ngang theo format Viá»‡t Nam: 12A-34567
    # TÃ¬m vá»‹ trÃ­ cuá»‘i cÃ¹ng cá»§a chá»¯ cÃ¡i
    last_letter_pos = -1
    for i, char in enumerate(text):
        if char.isalpha():
            last_letter_pos = i
    
    # Náº¿u tÃ¬m tháº¥y chá»¯ cÃ¡i vÃ  cÃ³ sá»‘ sau Ä‘Ã³, thÃªm dáº¥u gáº¡ch ngang
    if last_letter_pos >= 0 and last_letter_pos < len(text) - 1:
        formatted = text[:last_letter_pos + 1] + '-' + text[last_letter_pos + 1:]
        return formatted
    
    return text


def predict_single_image(model, image_tensor, config, device='cpu'):
    """
    Dá»± Ä‘oÃ¡n biá»ƒn sá»‘ xe tá»« 1 áº£nh
    
    Args:
        model: Model VOCR
        image_tensor: Tensor áº£nh Ä‘Ã£ tiá»n xá»­ lÃ½
        config: Configuration
        device: Device Ä‘á»ƒ cháº¡y
        
    Returns:
        str: Biá»ƒn sá»‘ xe Ä‘Æ°á»£c dá»± Ä‘oÃ¡n
    """
    try:
        # Chuyá»ƒn model vÃ  tensor lÃªn device
        model = model.to(device)
        image_tensor = image_tensor.to(device)
        
        # Set model to evaluation mode
        model.eval()
        
        with torch.no_grad():
            # Forward pass
            outputs = model(image_tensor, targets=None, teacher_forcing_ratio=0.0)
            logits = outputs['outputs']  # [1, seq_len, num_classes]
            
            # Get predicted indices
            _, predicted_indices = torch.max(logits, dim=2)  # [1, seq_len]
            
            # Decode to string
            indices = predicted_indices[0].cpu().tolist()
            chars = []
            
            idx_to_char = config.get('data.idx_to_char', {})
            
            for idx in indices:
                if idx == 0:  # <pad>
                    continue
                elif idx == 1:  # <sos>
                    continue
                elif idx == 2:  # <eos>
                    break
                elif idx in idx_to_char:
                    chars.append(idx_to_char[idx])
            
            prediction = ''.join(chars)
            
            # Format vá»›i dáº¥u gáº¡ch ngang
            formatted_prediction = format_license_plate(prediction)
            
            print(f"ğŸ”¤ Raw prediction: '{prediction}'")
            print(f"ğŸ“‹ Formatted prediction: '{formatted_prediction}'")
            
            return formatted_prediction, outputs.get('attention_weights')
            
    except Exception as e:
        print(f"âŒ Error during prediction: {e}")
        return "", None


def visualize_result(original_image, prediction, attention_weights=None, save_path=None):
    """
    Hiá»ƒn thá»‹ káº¿t quáº£ dá»± Ä‘oÃ¡n
    
    Args:
        original_image: áº¢nh gá»‘c
        prediction: Káº¿t quáº£ dá»± Ä‘oÃ¡n
        attention_weights: Attention weights (optional)
        save_path: ÄÆ°á»ng dáº«n lÆ°u hÃ¬nh (optional)
    """
    try:
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Hiá»ƒn thá»‹ áº£nh gá»‘c
        axes[0].imshow(original_image, cmap='gray')
        axes[0].set_title(f'Input Image', fontsize=14)
        axes[0].axis('off')
        
        # Hiá»ƒn thá»‹ káº¿t quáº£
        axes[1].text(0.5, 0.7, 'Prediction:', ha='center', va='center', 
                    fontsize=16, fontweight='bold', transform=axes[1].transAxes)
        axes[1].text(0.5, 0.4, f"'{prediction}'", ha='center', va='center', 
                    fontsize=20, color='red', fontweight='bold', 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.7),
                    transform=axes[1].transAxes)
        
        axes[1].set_xlim(0, 1)
        axes[1].set_ylim(0, 1)
        axes[1].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ Result saved to: {save_path}")
        
        plt.show()
        
    except Exception as e:
        print(f"âŒ Error visualizing result: {e}")


def test_multiple_images(model, image_folder, config, device='cpu', max_images=10):
    """
    Test model vá»›i nhiá»u áº£nh trong folder
    
    Args:
        model: Model VOCR
        image_folder: ÄÆ°á»ng dáº«n folder chá»©a áº£nh
        config: Configuration
        device: Device
        max_images: Sá»‘ lÆ°á»£ng áº£nh tá»‘i Ä‘a Ä‘á»ƒ test
    """
    try:
        # Láº¥y danh sÃ¡ch file áº£nh
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        image_files = []
        
        for ext in image_extensions:
            image_files.extend(Path(image_folder).glob(f'*{ext}'))
            image_files.extend(Path(image_folder).glob(f'*{ext.upper()}'))
        
        image_files = sorted(image_files)[:max_images]
        
        if not image_files:
            print(f"âŒ No images found in {image_folder}")
            return
        
        print(f"ğŸ” Found {len(image_files)} images to test")
        
        results = []
        
        for i, image_path in enumerate(image_files):
            print(f"\n{'='*50}")
            print(f"ğŸ“· Testing image {i+1}/{len(image_files)}: {image_path.name}")
            print('='*50)
            
            # Preprocess image
            image_tensor, original_image = preprocess_single_image(str(image_path))
            
            if image_tensor is not None:
                # Predict
                prediction, attention_weights = predict_single_image(
                    model, image_tensor, config, device
                )
                
                results.append({
                    'image_path': str(image_path),
                    'prediction': prediction
                })
                
                print(f"âœ… Result: {image_path.name} -> '{prediction}'")
            else:
                print(f"âŒ Failed to process: {image_path.name}")
        
        # Summary
        print(f"\n{'='*60}")
        print("ğŸ“Š SUMMARY RESULTS")
        print('='*60)
        for i, result in enumerate(results):
            print(f"{i+1:2d}. {Path(result['image_path']).name:<30} -> '{result['prediction']}'")
        
        return results
        
    except Exception as e:
        print(f"âŒ Error testing multiple images: {e}")
        return []


def main():
    """
    HÃ m main
    """
    parser = argparse.ArgumentParser(description='Test VOCR Model with Single Image')
    parser.add_argument('--image', type=str, required=True,
                       help='Path to input image or folder')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='Path to model checkpoint')
    parser.add_argument('--config', type=str, default=None,
                       help='Path to config file')
    parser.add_argument('--device', type=str, default='auto',
                       help='Device to use (cpu, cuda, auto)')
    parser.add_argument('--save', type=str, default=None,
                       help='Path to save result image')
    parser.add_argument('--batch', action='store_true',
                       help='Test multiple images in folder')
    
    args = parser.parse_args()
    
    # Load config
    if args.config:
        config = load_config(args.config)
    else:
        config = get_config()
    
    print("ğŸš€ VOCR Model Testing")
    print("="*50)
    
    # Setup device
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f"ğŸ’» Using device: {device}")
    
    # Load model
    print(f"ğŸ“‚ Loading model from: {args.checkpoint}")
    model = load_trained_model(args.checkpoint, config)
    
    if model is None:
        print("âŒ Failed to load model. Exiting...")
        return
    
    model = model.to(device)
    
    # Test mode: single image or batch
    if args.batch or os.path.isdir(args.image):
        print(f"ğŸ“ Batch testing mode: {args.image}")
        results = test_multiple_images(model, args.image, config, device)
        
    else:
        print(f"ğŸ–¼ï¸  Single image testing: {args.image}")
        
        # Preprocess image
        print("\nğŸ”„ Preprocessing image...")
        image_tensor, original_image = preprocess_single_image(args.image)
        
        if image_tensor is None:
            print("âŒ Failed to preprocess image. Exiting...")
            return
        
        # Predict
        print("\nğŸ§  Running prediction...")
        prediction, attention_weights = predict_single_image(
            model, image_tensor, config, device
        )
        
        # Display results
        print(f"\nğŸ¯ FINAL RESULT: '{prediction}'")
        
        # Visualize
        print("\nğŸ“Š Visualizing result...")
        save_path = args.save if args.save else None
        visualize_result(original_image, prediction, attention_weights, save_path)
        
        # Additional info
        print(f"\nğŸ“‹ Prediction details:")
        print(f"   - Length: {len(prediction)} characters")
        print(f"   - Format: Vietnamese license plate")
        print(f"   - Confidence: Model prediction (no confidence score available)")
    
    print("\nâœ… Testing completed!")


if __name__ == '__main__':
    main()